# StatefulSet

## Task 1

`helm install --dry-run --debug python . --values values.python.yaml`

```
NAME: python
LAST DEPLOYED: Tue Nov 28 16:45:56 2023
NAMESPACE: default
STATUS: pending-install
REVISION: 1
USER-SUPPLIED VALUES:
PVClaim:
  accessModes:
  - ReadWriteOnce
  name: python-visits
  size: 1Mi
affinity: {}
autoscaling:
  enabled: false
  maxReplicas: 100
  minReplicas: 1
  targetCPUUtilizationPercentage: 80
fullnameOverride: ""
image:
  pullPolicy: IfNotPresent
  repository: annadluzhinskaya/python-moscow-time
  tag: latest
imagePullSecrets: []
ingress:
  annotations: {}
  className: ""
  enabled: false
  hosts:
  - host: chart-example.local
    paths:
    - path: /
      pathType: ImplementationSpecific
  tls: []
nameOverride: ""
nodeSelector: {}
podAnnotations:
  vault.hashicorp.com/agent-inject: "true"
  vault.hashicorp.com/agent-inject-secret-database-config.txt: internal/data/database/config
  vault.hashicorp.com/role: internal-app
podLabels: {}
podSecurityContext: {}
replicaCount: 2
resources:
  limits:
    cpu: 128m
    memory: 256Mi
  requests:
    cpu: 128m
    memory: 256Mi
securityContext: {}
service:
  port: 8080
  type: LoadBalancer
serviceAccount:
  annotations: {}
  automount: true
  create: true
  name: internal-app
tolerations: []
volumeMounts:
- mountPath: /config.json
  name: config
  readOnly: true
  subPath: config.json
- mountPath: /app_python/volume/
  name: python-visits
volumes:
- configMap:
    name: simple-configmap
  name: config

COMPUTED VALUES:
PVClaim:
  accessModes:
  - ReadWriteOnce
  name: python-visits
  size: 1Mi
affinity: {}
autoscaling:
  enabled: false
  maxReplicas: 100
  minReplicas: 1
  targetCPUUtilizationPercentage: 80
fullnameOverride: ""
image:
  pullPolicy: IfNotPresent
  repository: annadluzhinskaya/python-moscow-time
  tag: latest
imagePullSecrets: []
ingress:
  annotations: {}
  className: ""
  enabled: false
  hosts:
  - host: chart-example.local
    paths:
    - path: /
      pathType: ImplementationSpecific
  tls: []
mylibchart:
  global: {}
nameOverride: ""
nodeSelector: {}
podAnnotations:
  vault.hashicorp.com/agent-inject: "true"
  vault.hashicorp.com/agent-inject-secret-database-config.txt: internal/data/database/config
  vault.hashicorp.com/role: internal-app
podLabels: {}
podSecurityContext: {}
replicaCount: 2
resources:
  limits:
    cpu: 128m
    memory: 256Mi
  requests:
    cpu: 128m
    memory: 256Mi
securityContext: {}
service:
  port: 8080
  type: LoadBalancer
serviceAccount:
  annotations: {}
  automount: true
  create: true
  name: internal-app
tolerations: []
volumeMounts:
- mountPath: /config.json
  name: config
  readOnly: true
  subPath: config.json
- mountPath: /app_python/volume/
  name: python-visits
volumes:
- configMap:
    name: simple-configmap
  name: config

HOOKS:
---
# Source: helm-app/templates/post-install-job.yaml
apiVersion: v1
kind: Pod
metadata:
  name: postinstall-hook
  annotations:
    "helm.sh/hook": "post-install"
    "helm.sh/hook-delete-policy": hook-succeeded
spec:
  containers:
  - name: post-install-container
    image: busybox
    imagePullPolicy: IfNotPresent
    command: ['sh', '-c', 'echo The post-install hook is running && sleep 20' ]
  restartPolicy: Never
  terminationGracePeriodSeconds: 0
---
# Source: helm-app/templates/pre-install-job.yaml
apiVersion: v1
kind: Pod
metadata:
  name: preinstall-hook
  annotations:
    "helm.sh/hook": "pre-install"
    "helm.sh/hook-delete-policy": hook-succeeded
spec:
  containers:
  - name: pre-install-container
    image: busybox
    imagePullPolicy: IfNotPresent
    command: ['sh', '-c', 'echo The pre-install hook is running && sleep 20' ]
  restartPolicy: Never
  terminationGracePeriodSeconds: 0
---
# Source: helm-app/templates/tests/test-connection.yaml
apiVersion: v1
kind: Pod
metadata:
  name: "python-helm-app-test-connection"
  labels:
    helm.sh/chart: helm-app-0.1.0
    app.kubernetes.io/name: helm-app
    app.kubernetes.io/instance: python
    app.kubernetes.io/version: "1.16.0"
    app.kubernetes.io/managed-by: Helm
  annotations:
    "helm.sh/hook": test
spec:
  containers:
    - name: wget
      image: busybox
      command: ['wget']
      args: ['python-helm-app:8080']
  restartPolicy: Never
MANIFEST:
---
# Source: helm-app/templates/serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: internal-app
  labels:
    helm.sh/chart: helm-app-0.1.0
    app.kubernetes.io/name: helm-app
    app.kubernetes.io/instance: python
    app.kubernetes.io/version: "1.16.0"
    app.kubernetes.io/managed-by: Helm
automountServiceAccountToken: true
---
# Source: helm-app/templates/secrets.yaml
apiVersion: v1
data:
  password: QU5OQQ==
metadata:
  name: secret-base
kind: Secret
type: Opaque
---
# Source: helm-app/templates/configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: simple-configmap
data:
  config.json: |-
    {"abc":"def"}
---
# Source: helm-app/templates/env-configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: simple-env-configmap
data:
  ABC: "DEF"
---
# Source: helm-app/templates/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: python-helm-app
  labels:
    helm.sh/chart: helm-app-0.1.0
    app.kubernetes.io/name: helm-app
    app.kubernetes.io/instance: python
    app.kubernetes.io/version: "1.16.0"
    app.kubernetes.io/managed-by: Helm
spec:
  type: LoadBalancer
  ports:
    - port: 8080
      targetPort: http
      protocol: TCP
      name: http
  selector:
    app.kubernetes.io/name: helm-app
    app.kubernetes.io/instance: python
---
# Source: helm-app/templates/statefulset.yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: python-helm-app
  labels:
    helm.sh/chart: helm-app-0.1.0
    app.kubernetes.io/name: helm-app
    app.kubernetes.io/instance: python
    app.kubernetes.io/version: "1.16.0"
    app.kubernetes.io/managed-by: Helm
spec:
  replicas: 2
  selector:
    matchLabels:
      app.kubernetes.io/name: helm-app
      app.kubernetes.io/instance: python
  template:
    metadata:
      annotations:
        vault.hashicorp.com/agent-inject: "true"
        vault.hashicorp.com/agent-inject-secret-database-config.txt: internal/data/database/config
        vault.hashicorp.com/role: internal-app
      labels:
        helm.sh/chart: helm-app-0.1.0
        app.kubernetes.io/name: helm-app
        app.kubernetes.io/instance: python
        app.kubernetes.io/version: "1.16.0"
        app.kubernetes.io/managed-by: Helm
    spec:
      serviceAccountName: internal-app
      securityContext:
        {}
      containers:
        - name: helm-app
          securityContext:
            {}
          image: "annadluzhinskaya/python-moscow-time:latest"
          imagePullPolicy: IfNotPresent
          ports:
            - name: http
              containerPort: 8080
              protocol: TCP
          env:
            - name: ENV-CONFIGMAP
              valueFrom:
                configMapKeyRef:
                  name: simple-env-configmap
                  key: ABC
            - name: PASSWORD
              valueFrom:
                secretKeyRef:
                  key: password
                  name: secret-base
            - name: VAR_2
              value: "ABC"
            - name: VAR_1
              value: "DEF"
          livenessProbe:
            httpGet:
              path: /healthz
              port: http
          readinessProbe:
            httpGet:
              path: /healthz
              port: http
          resources:
            limits:
              cpu: 128m
              memory: 256Mi
            requests:
              cpu: 128m
              memory: 256Mi
          volumeMounts:
            - mountPath: /config.json
              name: config
              readOnly: true
              subPath: config.json
            - mountPath: /app_python/volume/
              name: python-visits
      volumes:
        - configMap:
            name: simple-configmap
          name: config
  volumeClaimTemplates:
    - metadata:
        name: python-visits
      spec:
        accessModes: [ReadWriteOnce]
        resources:
          requests:
            storage: 1Mi

NOTES:
1. Get the application URL by running these commands:
     NOTE: It may take a few minutes for the LoadBalancer IP to be available.
           You can watch the status of by running 'kubectl get --namespace default svc -w python-helm-app'
  export SERVICE_IP=$(kubectl get svc --namespace default python-helm-app --template "{{ range (index .status.loadBalancer.ingress 0) }}{{.}}{{ end }}")
  echo http://$SERVICE_IP:8080
```

## Task 2

### StatefulSet

`helm install python . --values values.python.yaml`

```
NAME: python
LAST DEPLOYED: Tue Nov 28 16:47:06 2023
NAMESPACE: default
STATUS: deployed
REVISION: 1
NOTES:
1. Get the application URL by running these commands:
     NOTE: It may take a few minutes for the LoadBalancer IP to be available.
           You can watch the status of by running 'kubectl get --namespace default svc -w python-helm-app'
  export SERVICE_IP=$(kubectl get svc --namespace default python-helm-app --template "{{ range (index .status.loadBalancer.ingress 0) }}{{.}}{{ end }}")
  echo http://$SERVICE_IP:8080
```

`kubectl get po,sts,svc,pvc`

```
NAME                    READY   STATUS    RESTARTS   AGE
pod/python-helm-app-0   1/1     Running   0          56s
pod/python-helm-app-1   1/1     Running   0          34s

NAME                               READY   AGE
statefulset.apps/python-helm-app   2/2     56s

NAME                      TYPE           CLUSTER-IP      EXTERNAL-IP   PORT(S)          AGE
service/kubernetes        ClusterIP      10.96.0.1       <none>        443/TCP          27d
service/python-helm-app   LoadBalancer   10.106.153.44   <pending>     8080:31733/TCP   56s

NAME                                                    STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS   AGE
persistentvolumeclaim/data-vault-0                      Bound    pvc-65d22614-70c0-4729-8041-8976790918ab   10Gi       RWO            standard       13d
persistentvolumeclaim/python-visits-python-helm-app-0   Bound    pvc-9b0f74d4-e6af-4c8c-8b00-eb629cffe57e   1Mi        RWO            standard       56s
persistentvolumeclaim/python-visits-python-helm-app-1   Bound    pvc-b24f9554-4b2b-4470-899f-6727aafed7a4   1Mi        RWO            standard       34s
```

`minikube service python-helm-app`

```
|-----------|-----------------|-------------|---------------------------|
| NAMESPACE |      NAME       | TARGET PORT |            URL            |
|-----------|-----------------|-------------|---------------------------|
| default   | python-helm-app | http/8080   | http://192.168.49.2:31733 |
|-----------|-----------------|-------------|---------------------------|
🏃  Starting tunnel for service python-helm-app.
|-----------|-----------------|-------------|------------------------|
| NAMESPACE |      NAME       | TARGET PORT |          URL           |
|-----------|-----------------|-------------|------------------------|
| default   | python-helm-app |             | http://127.0.0.1:62531 |
|-----------|-----------------|-------------|------------------------|
🎉  Opening service default/python-helm-app in default browser...
❗  Because you are using a Docker driver on darwin, the terminal needs to be open to run it.
```

`kubectl exec python-helm-app-0 -- cat /app_python/volume/visits`

```
7
```

`kubectl exec python-helm-app-1 -- cat /app_python/volume/visits`

```
12
```

### Explain differences
Nodes has its own persistent volume. 
So, we can have different number of visits for different nodes.

### Parallel
Added `podManagementPolicy: Parallel` in `statefulset.yaml`

Why ordering guarantees are unnecessary for my app?
- Stable Network Identities: Each pod in a StatefulSet is uniquely identified and has stable network identities.
- Resumable Operations: StatefulSet pods can retrieve their prior state from storage and continue operations, even after being rescheduled to different nodes.

`kubectl get pods`

```
NAME                READY   STATUS    RESTARTS   AGE
python-helm-app-0   1/1     Running   0          29s
python-helm-app-1   1/1     Running   0          29s
```

## Bonus

### Extra app

`helm install --dry-run --debug sharp . --values values.sharp.yaml`

```
NAME: sharp
LAST DEPLOYED: Tue Nov 28 16:58:30 2023
NAMESPACE: default
STATUS: pending-install
REVISION: 1
USER-SUPPLIED VALUES:
PVClaim:
  accessModes:
  - ReadWriteOnce
  name: c-sharp-visits
  size: 1Mi
affinity: {}
autoscaling:
  enabled: false
  maxReplicas: 100
  minReplicas: 1
  targetCPUUtilizationPercentage: 80
fullnameOverride: ""
image:
  pullPolicy: IfNotPresent
  repository: annadluzhinskaya/pet-app
  tag: latest
imagePullSecrets: []
ingress:
  annotations: {}
  className: ""
  enabled: false
  hosts:
  - host: chart-example.local
    paths:
    - path: /
      pathType: ImplementationSpecific
  tls: []
nameOverride: ""
nodeSelector: {}
podAnnotations:
  vault.hashicorp.com/agent-inject: "true"
  vault.hashicorp.com/agent-inject-secret-database-config.txt: internal/data/database/config
  vault.hashicorp.com/role: internal-app
podLabels: {}
podSecurityContext: {}
replicaCount: 2
resources:
  limits:
    cpu: 128m
    memory: 256Mi
  requests:
    cpu: 128m
    memory: 256Mi
securityContext: {}
service:
  port: 80
  type: LoadBalancer
serviceAccount:
  annotations: {}
  automount: true
  create: true
  name: internal-app
tolerations: []
volumeMounts:
- mountPath: /config.json
  name: config
  readOnly: true
  subPath: config.json
- mountPath: /app_c_sharp/volume/
  name: c-sharp-visits
volumes:
- configMap:
    name: simple-configmap
  name: config

COMPUTED VALUES:
PVClaim:
  accessModes:
  - ReadWriteOnce
  name: c-sharp-visits
  size: 1Mi
affinity: {}
autoscaling:
  enabled: false
  maxReplicas: 100
  minReplicas: 1
  targetCPUUtilizationPercentage: 80
fullnameOverride: ""
image:
  pullPolicy: IfNotPresent
  repository: annadluzhinskaya/pet-app
  tag: latest
imagePullSecrets: []
ingress:
  annotations: {}
  className: ""
  enabled: false
  hosts:
  - host: chart-example.local
    paths:
    - path: /
      pathType: ImplementationSpecific
  tls: []
mylibchart:
  global: {}
nameOverride: ""
nodeSelector: {}
podAnnotations:
  vault.hashicorp.com/agent-inject: "true"
  vault.hashicorp.com/agent-inject-secret-database-config.txt: internal/data/database/config
  vault.hashicorp.com/role: internal-app
podLabels: {}
podSecurityContext: {}
replicaCount: 2
resources:
  limits:
    cpu: 128m
    memory: 256Mi
  requests:
    cpu: 128m
    memory: 256Mi
securityContext: {}
service:
  port: 80
  type: LoadBalancer
serviceAccount:
  annotations: {}
  automount: true
  create: true
  name: internal-app
tolerations: []
volumeMounts:
- mountPath: /config.json
  name: config
  readOnly: true
  subPath: config.json
- mountPath: /app_c_sharp/volume/
  name: c-sharp-visits
volumes:
- configMap:
    name: simple-configmap
  name: config

HOOKS:
---
# Source: helm-app/templates/post-install-job.yaml
apiVersion: v1
kind: Pod
metadata:
  name: postinstall-hook
  annotations:
    "helm.sh/hook": "post-install"
    "helm.sh/hook-delete-policy": hook-succeeded
spec:
  containers:
  - name: post-install-container
    image: busybox
    imagePullPolicy: IfNotPresent
    command: ['sh', '-c', 'echo The post-install hook is running && sleep 20' ]
  restartPolicy: Never
  terminationGracePeriodSeconds: 0
---
# Source: helm-app/templates/pre-install-job.yaml
apiVersion: v1
kind: Pod
metadata:
  name: preinstall-hook
  annotations:
    "helm.sh/hook": "pre-install"
    "helm.sh/hook-delete-policy": hook-succeeded
spec:
  containers:
  - name: pre-install-container
    image: busybox
    imagePullPolicy: IfNotPresent
    command: ['sh', '-c', 'echo The pre-install hook is running && sleep 20' ]
  restartPolicy: Never
  terminationGracePeriodSeconds: 0
---
# Source: helm-app/templates/tests/test-connection.yaml
apiVersion: v1
kind: Pod
metadata:
  name: "sharp-helm-app-test-connection"
  labels:
    helm.sh/chart: helm-app-0.1.0
    app.kubernetes.io/name: helm-app
    app.kubernetes.io/instance: sharp
    app.kubernetes.io/version: "1.16.0"
    app.kubernetes.io/managed-by: Helm
  annotations:
    "helm.sh/hook": test
spec:
  containers:
    - name: wget
      image: busybox
      command: ['wget']
      args: ['sharp-helm-app:80']
  restartPolicy: Never
MANIFEST:
---
# Source: helm-app/templates/serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: internal-app
  labels:
    helm.sh/chart: helm-app-0.1.0
    app.kubernetes.io/name: helm-app
    app.kubernetes.io/instance: sharp
    app.kubernetes.io/version: "1.16.0"
    app.kubernetes.io/managed-by: Helm
automountServiceAccountToken: true
---
# Source: helm-app/templates/secrets.yaml
apiVersion: v1
data:
  password: QU5OQQ==
metadata:
  name: secret-base
kind: Secret
type: Opaque
---
# Source: helm-app/templates/configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: simple-configmap
data:
  config.json: |-
    {"abc":"def"}
---
# Source: helm-app/templates/env-configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: simple-env-configmap
data:
  ABC: "DEF"
---
# Source: helm-app/templates/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: sharp-helm-app
  labels:
    helm.sh/chart: helm-app-0.1.0
    app.kubernetes.io/name: helm-app
    app.kubernetes.io/instance: sharp
    app.kubernetes.io/version: "1.16.0"
    app.kubernetes.io/managed-by: Helm
spec:
  type: LoadBalancer
  ports:
    - port: 80
      targetPort: http
      protocol: TCP
      name: http
  selector:
    app.kubernetes.io/name: helm-app
    app.kubernetes.io/instance: sharp
---
# Source: helm-app/templates/statefulset.yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: sharp-helm-app
  labels:
    helm.sh/chart: helm-app-0.1.0
    app.kubernetes.io/name: helm-app
    app.kubernetes.io/instance: sharp
    app.kubernetes.io/version: "1.16.0"
    app.kubernetes.io/managed-by: Helm
spec:
  podManagementPolicy: Parallel
  replicas: 2
  selector:
    matchLabels:
      app.kubernetes.io/name: helm-app
      app.kubernetes.io/instance: sharp
  template:
    metadata:
      annotations:
        vault.hashicorp.com/agent-inject: "true"
        vault.hashicorp.com/agent-inject-secret-database-config.txt: internal/data/database/config
        vault.hashicorp.com/role: internal-app
      labels:
        helm.sh/chart: helm-app-0.1.0
        app.kubernetes.io/name: helm-app
        app.kubernetes.io/instance: sharp
        app.kubernetes.io/version: "1.16.0"
        app.kubernetes.io/managed-by: Helm
    spec:
      serviceAccountName: internal-app
      securityContext:
        {}
      containers:
        - name: helm-app
          securityContext:
            {}
          image: "annadluzhinskaya/pet-app:latest"
          imagePullPolicy: IfNotPresent
          ports:
            - name: http
              containerPort: 80
              protocol: TCP
          env:
            - name: ENV-CONFIGMAP
              valueFrom:
                configMapKeyRef:
                  name: simple-env-configmap
                  key: ABC
            - name: PASSWORD
              valueFrom:
                secretKeyRef:
                  key: password
                  name: secret-base
            - name: VAR_2
              value: "ABC"
            - name: VAR_1
              value: "DEF"
          livenessProbe:
            httpGet:
              path: /healthz
              port: http
          readinessProbe:
            httpGet:
              path: /healthz
              port: http
          resources:
            limits:
              cpu: 128m
              memory: 256Mi
            requests:
              cpu: 128m
              memory: 256Mi
          volumeMounts:
            - mountPath: /config.json
              name: config
              readOnly: true
              subPath: config.json
            - mountPath: /app_c_sharp/volume/
              name: c-sharp-visits
      volumes:
        - configMap:
            name: simple-configmap
          name: config
  volumeClaimTemplates:
    - metadata:
        name: c-sharp-visits
      spec:
        accessModes: [ReadWriteOnce]
        resources:
          requests:
            storage: 1Mi

NOTES:
1. Get the application URL by running these commands:
     NOTE: It may take a few minutes for the LoadBalancer IP to be available.
           You can watch the status of by running 'kubectl get --namespace default svc -w sharp-helm-app'
  export SERVICE_IP=$(kubectl get svc --namespace default sharp-helm-app --template "{{ range (index .status.loadBalancer.ingress 0) }}{{.}}{{ end }}")
  echo http://$SERVICE_IP:80
```

`helm install sharp . --values values.sharp.yaml`

```
NAME: sharp
LAST DEPLOYED: Tue Nov 28 16:59:53 2023
NAMESPACE: default
STATUS: deployed
REVISION: 1
NOTES:
1. Get the application URL by running these commands:
     NOTE: It may take a few minutes for the LoadBalancer IP to be available.
           You can watch the status of by running 'kubectl get --namespace default svc -w sharp-helm-app'
  export SERVICE_IP=$(kubectl get svc --namespace default sharp-helm-app --template "{{ range (index .status.loadBalancer.ingress 0) }}{{.}}{{ end }}")
  echo http://$SERVICE_IP:80
```

`kubectl get po,sts,svc,pvc`

```
NAME                   READY   STATUS    RESTARTS   AGE
pod/sharp-helm-app-0   1/1     Running   0          41s
pod/sharp-helm-app-1   1/1     Running   0          41s

NAME                              READY   AGE
statefulset.apps/sharp-helm-app   2/2     41s

NAME                     TYPE           CLUSTER-IP      EXTERNAL-IP   PORT(S)        AGE
service/kubernetes       ClusterIP      10.96.0.1       <none>        443/TCP        27d
service/sharp-helm-app   LoadBalancer   10.109.121.73   <pending>     80:31187/TCP   41s

NAME                                                    STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS   AGE
persistentvolumeclaim/c-sharp-visits-sharp-helm-app-0   Bound    pvc-f3c04261-4d12-47e1-b471-cf8fc61c57c1   1Mi        RWO            standard       41s
persistentvolumeclaim/c-sharp-visits-sharp-helm-app-1   Bound    pvc-8a25141d-9ec8-41c9-99e8-e63a28985dca   1Mi        RWO            standard       41s
persistentvolumeclaim/data-vault-0                      Bound    pvc-65d22614-70c0-4729-8041-8976790918ab   10Gi       RWO            standard       13d
persistentvolumeclaim/python-visits-python-helm-app-0   Bound    pvc-9b0f74d4-e6af-4c8c-8b00-eb629cffe57e   1Mi        RWO            standard       13m
persistentvolumeclaim/python-visits-python-helm-app-1   Bound    pvc-b24f9554-4b2b-4470-899f-6727aafed7a4   1Mi        RWO            standard       13m
```

`minikube service sharp-helm-app`

```
|-----------|----------------|-------------|---------------------------|
| NAMESPACE |      NAME      | TARGET PORT |            URL            |
|-----------|----------------|-------------|---------------------------|
| default   | sharp-helm-app | http/80     | http://192.168.49.2:31187 |
|-----------|----------------|-------------|---------------------------|
🏃  Starting tunnel for service sharp-helm-app.
|-----------|----------------|-------------|------------------------|
| NAMESPACE |      NAME      | TARGET PORT |          URL           |
|-----------|----------------|-------------|------------------------|
| default   | sharp-helm-app |             | http://127.0.0.1:63354 |
|-----------|----------------|-------------|------------------------|
🎉  Opening service default/sharp-helm-app in default browser...
❗  Because you are using a Docker driver on darwin, the terminal needs to be open to run it.
```

`kubectl exec sharp-helm-app-0 -- cat /app_c_sharp/volume/visits`

```
37
```

`kubectl exec sharp-helm-app-1 -- cat /app_c_sharp/volume/visits`

```
7
```

From part with `kubectl get po,sts,svc,pvc` you can see that extra app starts in parallel.

### Update strategies for Kubernetes StatefulSet
- Rolling Update: This strategy updates one pod at a time within the StatefulSet, ensuring that the application remains available throughout the update process. Once a new pod is successfully running, the next one is updated, and the process continues until all pods are updated.
- OnDelete Update: This strategy allows users to manually control the update process. Pods are not automatically replaced, and the user must delete each pod in the StatefulSet, triggering the creation of their updated versions.
- Parallel Update: In this strategy, multiple pods within the StatefulSet can be updated simultaneously, enabling faster updates. However, it is crucial to ensure that the application can handle potential downtime or disruptions resulting from simultaneous updates.

Each of these strategies offers different trade-offs in terms of update speed, availability, and control, allowing users to choose the most suitable approach based on their specific application requirements.
